---
title: Production Deployment
description: Deploy ImportCSV to production with confidence
---

## Environment Setup

### Environment Variables

```bash
# .env.local (Next.js)
NEXT_PUBLIC_MAX_FILE_SIZE=10485760  # 10MB
NEXT_PUBLIC_MAX_ROWS=50000          # 50k rows
```

### File Size Limits

Set reasonable limits based on your use case:

```tsx
<CSVImporter
  schema={schema}
  onComplete={handleImport}
  maxFileSize={10 * 1024 * 1024}  // 10MB
  maxRows={50000}                  // 50k rows
/>
```

## Performance Optimization

### Large Dataset Handling

For imports > 10k rows:

```tsx
<CSVImporter
  schema={schema}
  onComplete={handleImport}
  // Enable streaming for large files
  streamingThreshold={5000}
  // Virtual scrolling automatically enabled
/>
```

See [Performance Guide](/advanced/performance) for details.

### Code Splitting

Dynamically import to reduce initial bundle:

```tsx
// Next.js
import dynamic from 'next/dynamic';

const CSVImporter = dynamic(
  () => import('@importcsv/react').then(mod => mod.CSVImporter),
  { ssr: false }
);
```

```tsx
// React (Vite/CRA)
import { lazy, Suspense } from 'react';

const CSVImporter = lazy(() =>
  import('@importcsv/react').then(mod => ({ default: mod.CSVImporter }))
);

function App() {
  return (
    <Suspense fallback={<div>Loading...</div>}>
      <CSVImporter schema={schema} onComplete={handleImport} />
    </Suspense>
  );
}
```

## Security

### Input Validation

Always validate on backend:

```tsx
// Frontend (CSVImporter does initial validation)
<CSVImporter schema={frontendSchema} onComplete={handleImport} />

// Backend (validate again)
export async function POST(request: Request) {
  const { users } = await request.json();

  // Re-validate with backend schema
  const validated = users.map(u => backendSchema.parse(u));

  // Safe to insert
  await db.user.createMany({ data: validated });
}
```

### File Type Restrictions

ImportCSV only accepts CSV/Excel files by default. No additional configuration needed.

### Rate Limiting

Protect your import endpoint:

```tsx
// Next.js with rate limiting
import { rateLimit } from '@/lib/rate-limit';

const limiter = rateLimit({
  interval: 60 * 1000, // 1 minute
  uniqueTokenPerInterval: 500
});

export async function POST(request: Request) {
  try {
    await limiter.check(10, 'IMPORT'); // 10 requests per minute
  } catch {
    return NextResponse.json(
      { error: 'Rate limit exceeded' },
      { status: 429 }
    );
  }

  // Handle import...
}
```

## Error Monitoring

### Sentry Integration

```tsx
import * as Sentry from '@sentry/nextjs';

function CustomerImporter() {
  const handleImport = async (customers: Customer[]) => {
    try {
      await fetch('/api/customers/import', {
        method: 'POST',
        body: JSON.stringify({ customers })
      });
    } catch (error) {
      Sentry.captureException(error, {
        tags: { feature: 'csv-import' },
        extra: { rowCount: customers.length }
      });
      throw error;
    }
  };

  return <CSVImporter schema={schema} onComplete={handleImport} />;
}
```

## Monitoring & Analytics

### Track Import Metrics

```tsx
function trackImport(data: any[], duration: number) {
  // Your analytics (PostHog, Mixpanel, etc.)
  analytics.track('CSV Import Completed', {
    row_count: data.length,
    duration_ms: duration,
    timestamp: new Date().toISOString()
  });
}

function CustomerImporter() {
  const handleImport = async (customers: Customer[]) => {
    const start = Date.now();

    try {
      await submitToAPI(customers);
      trackImport(customers, Date.now() - start);
    } catch (error) {
      analytics.track('CSV Import Failed', { error: error.message });
      throw error;
    }
  };

  return <CSVImporter schema={schema} onComplete={handleImport} />;
}
```

## Deployment Checklist

### Before Going Live

- [ ] Set file size limits appropriate for your use case
- [ ] Set maximum row limits to prevent abuse
- [ ] Add rate limiting to import endpoints
- [ ] Implement backend validation (don't trust frontend)
- [ ] Add error monitoring (Sentry, etc.)
- [ ] Test with large files (10k+ rows)
- [ ] Test error cases (invalid data, network failures)
- [ ] Configure code splitting for better load times
- [ ] Add analytics tracking for import events
- [ ] Document import limits for users

### Hosting Considerations

#### Vercel

- Default serverless timeout: 10s (Hobby), 60s (Pro)
- For large imports, use streaming or split into batches
- Edge runtime not supported (uses Node.js APIs)

#### Netlify

- Default function timeout: 10s (Free), 26s (Pro)
- Consider Netlify Background Functions for large imports

#### Traditional Hosting (AWS, GCP, etc.)

- No special configuration needed
- Set appropriate timeouts for your import size

## Common Production Issues

### Import Timing Out

**Problem**: Large imports timeout on serverless platforms

**Solution 1**: Batch processing
```tsx
async function handleLargeImport(data: User[]) {
  const batchSize = 100;
  for (let i = 0; i < data.length; i += batchSize) {
    await fetch('/api/users/import', {
      method: 'POST',
      body: JSON.stringify({ users: data.slice(i, i + batchSize) })
    });
  }
}
```

**Solution 2**: Background job
```tsx
// Frontend: Queue import job
const response = await fetch('/api/import/queue', {
  method: 'POST',
  body: JSON.stringify({ users: data })
});
const { jobId } = await response.json();

// Poll for completion
const result = await pollJobStatus(jobId);
```

### Memory Issues

**Problem**: Browser crashes with very large files

**Solution**: Set reasonable limits
```tsx
<CSVImporter
  schema={schema}
  maxRows={10000}  // Limit to 10k rows
  maxFileSize={5 * 1024 * 1024}  // 5MB max
/>
```

### Validation Too Strict

**Problem**: Users can't import valid data due to overly strict validation

**Solution**: Use `.optional()` and provide clear error messages
```tsx
const schema = z.object({
  email: z.string()
    .email('Must be a valid email address')
    .optional(),  // Allow empty emails

  phone: z.string()
    .regex(/^\+?[1-9]\d{1,14}$/, 'Must be valid E.164 format')
    .optional()
});
```

## Next Steps

- [Performance Guide](/advanced/performance) - Optimize for large datasets
- [Handling Data](/integration/handling-data) - Process imported data
- [API Reference](/reference/api) - All configuration options
